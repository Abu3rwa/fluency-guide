# Duplicate Receipt Upload Prevention Plan

## ðŸ“‹ Executive Summary

This plan outlines a comprehensive strategy to prevent duplicate receipt uploads in the payment system. The current system has basic duplicate detection but needs enhancement to handle various scenarios and edge cases.

## ðŸ” Current State Analysis

### âœ… **What's Already Working**

1. **Transaction ID Check**: Basic duplicate detection for transaction IDs
2. **Reference Number Check**: Duplicate detection for reference numbers per student
3. **File Upload Validation**: File type and size validation
4. **Basic Error Handling**: Error messages for duplicate submissions

### âŒ **Current Limitations**

1. **Limited Detection Methods**: Only checks transaction ID and reference number
2. **No File Hash Checking**: Doesn't detect duplicate files
3. **No OCR Integration**: Doesn't extract data from receipt images
4. **No Fuzzy Matching**: Exact matches only, no similarity detection
5. **No Time-based Deduplication**: No protection against rapid resubmissions
6. **No User Session Protection**: No prevention of accidental double-clicks

## ðŸŽ¯ **Comprehensive Prevention Strategy**

### **Phase 1: Enhanced Detection Methods**

#### 1. **File Hash-Based Detection**

```javascript
// Add to paymentService.js
import { createHash } from "crypto";

const calculateFileHash = async (file) => {
  return new Promise((resolve) => {
    const reader = new FileReader();
    reader.onload = (e) => {
      const hash = createHash("sha256").update(e.target.result).digest("hex");
      resolve(hash);
    };
    reader.readAsArrayBuffer(file);
  });
};

// Check for duplicate file hash
const checkDuplicateFile = async (fileHash, studentId) => {
  const existingPayment = await getDocs(
    query(
      collection(db, PAYMENTS_COLLECTION),
      where("fileHash", "==", fileHash),
      where("studentId", "==", studentId)
    )
  );
  return !existingPayment.empty;
};
```

#### 2. **OCR Data Extraction**

```javascript
// Add OCR service integration
const extractReceiptData = async (file) => {
  try {
    // Use Google Cloud Vision API or similar
    const formData = new FormData();
    formData.append("file", file);

    const response = await fetch("/api/ocr/extract", {
      method: "POST",
      body: formData,
    });

    const ocrData = await response.json();
    return {
      transactionId: ocrData.transactionId,
      amount: ocrData.amount,
      date: ocrData.date,
      bankName: ocrData.bankName,
      accountNumber: ocrData.accountNumber,
      referenceNumber: ocrData.referenceNumber,
    };
  } catch (error) {
    console.error("OCR extraction failed:", error);
    return null;
  }
};
```

#### 3. **Enhanced Duplicate Detection**

```javascript
// Enhanced duplicate checking
const checkForDuplicates = async (paymentData, ocrData) => {
  const checks = [];

  // 1. Transaction ID check
  if (paymentData.transactionId || ocrData?.transactionId) {
    const transactionId = paymentData.transactionId || ocrData.transactionId;
    checks.push(checkDuplicateTransactionId(transactionId));
  }

  // 2. Reference number check
  if (paymentData.referenceNumber) {
    checks.push(
      checkDuplicateReferenceNumber(
        paymentData.referenceNumber,
        paymentData.studentId
      )
    );
  }

  // 3. File hash check
  if (paymentData.fileHash) {
    checks.push(
      checkDuplicateFile(paymentData.fileHash, paymentData.studentId)
    );
  }

  // 4. Amount + Date combination check
  if (ocrData?.amount && ocrData?.date) {
    checks.push(
      checkDuplicateAmountDate(
        ocrData.amount,
        ocrData.date,
        paymentData.studentId
      )
    );
  }

  // 5. Bank account + amount check
  if (ocrData?.accountNumber && ocrData?.amount) {
    checks.push(
      checkDuplicateBankAmount(
        ocrData.accountNumber,
        ocrData.amount,
        paymentData.studentId
      )
    );
  }

  const results = await Promise.all(checks);
  return results.filter((result) => result.isDuplicate);
};
```

### **Phase 2: User Experience Enhancements**

#### 1. **Session-Based Protection**

```javascript
// Add to PaymentDialog.jsx
const [isSubmitting, setIsSubmitting] = useState(false);
const [submissionTimeout, setSubmissionTimeout] = useState(null);

const handleSubmit = async () => {
  if (isSubmitting) {
    setError("Please wait, submission in progress...");
    return;
  }

  setIsSubmitting(true);
  setSubmissionTimeout(setTimeout(() => setIsSubmitting(false), 10000)); // 10 second timeout

  try {
    // ... existing submission logic
  } finally {
    setIsSubmitting(false);
    if (submissionTimeout) clearTimeout(submissionTimeout);
  }
};
```

#### 2. **Real-time Duplicate Detection**

```javascript
// Add real-time checking as user types
const [duplicateWarnings, setDuplicateWarnings] = useState([]);

const checkReferenceNumberRealTime = async (referenceNumber) => {
  if (referenceNumber.length < 3) return;

  try {
    const duplicates = await checkDuplicateReferenceNumber(
      referenceNumber,
      userData.uid
    );
    if (duplicates.length > 0) {
      setDuplicateWarnings([
        {
          type: "referenceNumber",
          message: "This reference number has already been submitted",
          existingPayment: duplicates[0],
        },
      ]);
    } else {
      setDuplicateWarnings((prev) =>
        prev.filter((w) => w.type !== "referenceNumber")
      );
    }
  } catch (error) {
    console.error("Real-time check failed:", error);
  }
};
```

#### 3. **Smart File Validation**

```javascript
// Enhanced file validation
const validateReceiptFile = async (file) => {
  const errors = [];

  // Basic validation
  if (!file) {
    errors.push("Please select a file");
    return errors;
  }

  // File type validation
  const allowedTypes = [
    "image/jpeg",
    "image/png",
    "image/jpg",
    "application/pdf",
  ];
  if (!allowedTypes.includes(file.type)) {
    errors.push("Invalid file type. Please upload JPG, PNG, or PDF");
  }

  // File size validation
  if (file.size > 5 * 1024 * 1024) {
    errors.push("File size must be less than 5MB");
  }

  // File hash check for duplicates
  if (file.size > 0) {
    const fileHash = await calculateFileHash(file);
    const isDuplicate = await checkDuplicateFile(fileHash, userData.uid);
    if (isDuplicate) {
      errors.push("This receipt has already been uploaded");
    }
  }

  return errors;
};
```

### **Phase 3: Advanced Prevention Features**

#### 1. **Fuzzy Matching for Similar Receipts**

```javascript
// Implement similarity detection
const checkSimilarReceipts = async (paymentData, ocrData) => {
  const similarReceipts = [];

  // Check for similar amounts (Â±1 SDG)
  if (ocrData?.amount) {
    const amountRange = {
      min: ocrData.amount - 1,
      max: ocrData.amount + 1,
    };

    const similarAmounts = await getDocs(
      query(
        collection(db, PAYMENTS_COLLECTION),
        where("studentId", "==", paymentData.studentId),
        where("amount", ">=", amountRange.min),
        where("amount", "<=", amountRange.max)
      )
    );

    similarReceipts.push(...similarAmounts.docs);
  }

  // Check for same date submissions
  if (ocrData?.date) {
    const sameDate = await getDocs(
      query(
        collection(db, PAYMENTS_COLLECTION),
        where("studentId", "==", paymentData.studentId),
        where("receiptDate", "==", ocrData.date)
      )
    );

    similarReceipts.push(...sameDate.docs);
  }

  return similarReceipts;
};
```

#### 2. **Time-Based Rate Limiting**

```javascript
// Implement rate limiting
const RATE_LIMIT_WINDOW = 5 * 60 * 1000; // 5 minutes
const MAX_SUBMISSIONS_PER_WINDOW = 3;

const checkRateLimit = async (studentId) => {
  const fiveMinutesAgo = new Date(Date.now() - RATE_LIMIT_WINDOW);

  const recentSubmissions = await getDocs(
    query(
      collection(db, PAYMENTS_COLLECTION),
      where("studentId", "==", studentId),
      where("submittedAt", ">=", fiveMinutesAgo)
    )
  );

  if (recentSubmissions.size >= MAX_SUBMISSIONS_PER_WINDOW) {
    throw new Error(
      `Too many submissions. Please wait ${Math.ceil(
        RATE_LIMIT_WINDOW / 60000
      )} minutes before trying again.`
    );
  }
};
```

#### 3. **Receipt Image Analysis**

```javascript
// Add image analysis for better detection
const analyzeReceiptImage = async (file) => {
  try {
    // Use TensorFlow.js or similar for image analysis
    const analysis = await tf.loadLayersModel("/models/receipt-analyzer");
    const imageTensor = await preprocessImage(file);
    const prediction = await analysis.predict(imageTensor);

    return {
      confidence: prediction.confidence,
      isReceipt: prediction.isReceipt > 0.8,
      quality: prediction.quality,
      orientation: prediction.orientation,
    };
  } catch (error) {
    console.error("Image analysis failed:", error);
    return {
      confidence: 0,
      isReceipt: true,
      quality: "unknown",
      orientation: "unknown",
    };
  }
};
```

### **Phase 4: Database Schema Enhancements**

#### 1. **Enhanced Payment Collection Schema**

```javascript
// Updated payment record structure
const paymentRecord = {
  // ... existing fields ...

  // Enhanced duplicate prevention fields
  fileHash: fileHash,
  fileSize: receiptFile.size,
  fileType: receiptFile.type,

  // OCR extracted data
  ocrData: {
    transactionId: ocrData?.transactionId,
    amount: ocrData?.amount,
    date: ocrData?.date,
    bankName: ocrData?.bankName,
    accountNumber: ocrData?.accountNumber,
    referenceNumber: ocrData?.referenceNumber,
    confidence: ocrData?.confidence,
  },

  // Similarity detection
  similarityChecks: {
    similarAmounts: [],
    sameDateSubmissions: [],
    sameBankSubmissions: [],
  },

  // Rate limiting
  submissionCount: 1,
  lastSubmissionTime: serverTimestamp(),

  // Validation metadata
  validationStatus: "pending",
  validationChecks: {
    fileHashValid: true,
    ocrExtractionSuccess: ocrData ? true : false,
    duplicateCheckPassed: true,
    rateLimitPassed: true,
  },
};
```

#### 2. **Dedicated Duplicate Detection Collection**

```javascript
// Create a separate collection for tracking duplicates
const DUPLICATE_DETECTION_COLLECTION = "duplicate_detections";

const recordDuplicateAttempt = async (
  paymentData,
  duplicateType,
  existingPayment
) => {
  await addDoc(collection(db, DUPLICATE_DETECTION_COLLECTION), {
    studentId: paymentData.studentId,
    courseId: paymentData.courseId,
    duplicateType, // 'transactionId', 'referenceNumber', 'fileHash', etc.
    existingPaymentId: existingPayment.id,
    attemptedAt: serverTimestamp(),
    userAgent: navigator.userAgent,
    ipAddress: null, // Could be added if needed
    sessionId: sessionStorage.getItem("sessionId"),
  });
};
```

### **Phase 5: User Interface Improvements**

#### 1. **Enhanced Payment Dialog**

```javascript
// Add to PaymentDialog.jsx
const [duplicateWarnings, setDuplicateWarnings] = useState([]);
const [isAnalyzing, setIsAnalyzing] = useState(false);
const [analysisResults, setAnalysisResults] = useState(null);

// Real-time duplicate checking
useEffect(() => {
  if (referenceNumber.length >= 3) {
    const timeoutId = setTimeout(() => {
      checkReferenceNumberRealTime(referenceNumber);
    }, 500);
    return () => clearTimeout(timeoutId);
  }
}, [referenceNumber]);

// File analysis on upload
const handleFileUpload = async (event) => {
  const file = event.target.files[0];
  if (file) {
    setIsAnalyzing(true);

    try {
      // Validate file
      const validationErrors = await validateReceiptFile(file);
      if (validationErrors.length > 0) {
        setError(validationErrors.join(", "));
        return;
      }

      // Analyze file
      const analysis = await analyzeReceiptImage(file);
      setAnalysisResults(analysis);

      // Extract OCR data
      const ocrData = await extractReceiptData(file);

      setReceiptFile(file);
      setReceiptFileName(file.name);
      setError("");
    } catch (error) {
      setError("File analysis failed. Please try again.");
    } finally {
      setIsAnalyzing(false);
    }
  }
};
```

#### 2. **Duplicate Warning Components**

```javascript
// Create DuplicateWarning component
const DuplicateWarning = ({ warnings, onViewExisting }) => {
  if (warnings.length === 0) return null;

  return (
    <Alert severity="warning" sx={{ mb: 2 }}>
      <AlertTitle>Potential Duplicate Detected</AlertTitle>
      {warnings.map((warning, index) => (
        <Box key={index} sx={{ mt: 1 }}>
          <Typography variant="body2">{warning.message}</Typography>
          {warning.existingPayment && (
            <Button
              size="small"
              onClick={() => onViewExisting(warning.existingPayment)}
              sx={{ mt: 1 }}
            >
              View Existing Payment
            </Button>
          )}
        </Box>
      ))}
    </Alert>
  );
};
```

#### 3. **File Analysis Display**

```javascript
// Create FileAnalysis component
const FileAnalysis = ({ analysis, isAnalyzing }) => {
  if (isAnalyzing) {
    return (
      <Box display="flex" alignItems="center" gap={1} sx={{ mt: 1 }}>
        <CircularProgress size={16} />
        <Typography variant="caption">Analyzing receipt...</Typography>
      </Box>
    );
  }

  if (!analysis) return null;

  return (
    <Paper sx={{ p: 2, mt: 2, bgcolor: "grey.50" }}>
      <Typography variant="subtitle2" gutterBottom>
        Receipt Analysis
      </Typography>
      <Grid container spacing={1}>
        <Grid item xs={6}>
          <Typography variant="caption">Quality:</Typography>
          <Typography variant="body2">{analysis.quality}</Typography>
        </Grid>
        <Grid item xs={6}>
          <Typography variant="caption">Confidence:</Typography>
          <Typography variant="body2">
            {Math.round(analysis.confidence * 100)}%
          </Typography>
        </Grid>
      </Grid>
    </Paper>
  );
};
```

## ðŸš€ **Implementation Roadmap**

### **Week 1: Foundation**

- [ ] Implement file hash calculation
- [ ] Add enhanced duplicate detection methods
- [ ] Create duplicate detection collection
- [ ] Update payment service with new checks

### **Week 2: User Experience**

- [ ] Add session-based protection
- [ ] Implement real-time duplicate checking
- [ ] Create duplicate warning components
- [ ] Add file analysis display

### **Week 3: Advanced Features**

- [ ] Integrate OCR service
- [ ] Implement fuzzy matching
- [ ] Add rate limiting
- [ ] Create image analysis

### **Week 4: Testing & Optimization**

- [ ] Comprehensive testing
- [ ] Performance optimization
- [ ] Error handling improvements
- [ ] Documentation updates

## ðŸ“Š **Success Metrics**

- **Duplicate Prevention Rate**: >95% of duplicate uploads prevented
- **False Positive Rate**: <2% of legitimate uploads blocked
- **User Experience**: <3 second response time for duplicate checks
- **System Performance**: <1 second for file analysis
- **Error Rate**: <1% of uploads fail due to prevention system

## ðŸ”§ **Technical Requirements**

### **Dependencies**

- **OCR Service**: Google Cloud Vision API or similar
- **Image Analysis**: TensorFlow.js or similar
- **Hash Calculation**: Node.js crypto module
- **Rate Limiting**: Firebase Functions or similar

### **Firebase Configuration**

```javascript
// Required Firestore indexes
{
  "collectionGroup": "payments",
  "queryScope": "COLLECTION",
  "fields": [
    { "fieldPath": "fileHash", "order": "ASCENDING" },
    { "fieldPath": "studentId", "order": "ASCENDING" }
  ]
},
{
  "collectionGroup": "payments",
  "queryScope": "COLLECTION",
  "fields": [
    { "fieldPath": "studentId", "order": "ASCENDING" },
    { "fieldPath": "submittedAt", "order": "DESCENDING" }
  ]
}
```

## ðŸŽ¯ **Expected Outcomes**

1. **Eliminate Duplicate Uploads**: Prevent 95%+ of duplicate receipt submissions
2. **Improve User Experience**: Real-time feedback and clear error messages
3. **Reduce Admin Workload**: Automated duplicate detection and handling
4. **Enhance Data Quality**: Cleaner payment records with extracted data
5. **Increase System Reliability**: Robust error handling and validation

---

**This comprehensive plan will transform the payment system from basic duplicate detection to a sophisticated, user-friendly system that prevents duplicate uploads while maintaining excellent user experience.**
